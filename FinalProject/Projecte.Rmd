---
title: "AD_projecte"
author: "Pau Mateo"
date: "2024-05-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
data <- read.csv("HousingData3.csv")
```

```{r, warning=FALSE}
library(FactoMineR)
library(factoextra)
library(ggplot2)
```

# EXPLORATOY DATA ANALYSIS

```{r}
summary(data)
```

There are 14 attributes in each case of the dataset. They are:

CRIM - per capita crime rate by town

ZN - proportion of residential land zoned for lots over 25,000 sq.ft.

INDUS - proportion of non-retail business acres per town.

CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)

NOX - nitric oxides concentration (parts per 10 million)

RM - average number of rooms per dwelling

AGE - proportion of owner-occupied units built prior to 1940

DIS - weighted distances to five Boston employment centres

RAD - index of accessibility to radial highways

TAX - full-value property-tax rate per \$10,000

PTRATIO - pupil-teacher ratio by town

B - 1000(Bk - 0.63)\^2 where Bk is the proportion of blacks by town

LSTAT - % lower status of the population

MEDV - Median value of owner-occupied homes in \$1000's

```{r}
for (i in 1:14){
  print(sum(is.na(data[,i]))/length(data[,i])*100)
}

# no missing values !!
```

No tenim mv, però veiem que la variable CHAS, que hauria de ser binària, pren alguns valors entremig de 0 i 1 en algunes ocasions. Arreglem aquests errors:

```{r}
data[data$CHAS != 0.0 & data$CHAS != 1.0, ]
```

Representen una fracció molt petita del conjunt total de dades, però tot i així considere-mho com a MV:

```{r}
mv_cols <- c(153,207,377,389,466,480,483)
for(i in mv_cols){
  data[i,]$CHAS <- NA
}

print(sum(is.na(data$CHAS))) #perfecte!
data[mv_cols, c(1,2,3,4,5)]
```

MV imputation:

```{r, warning=FALSE}
library(VIM)
data_imputed <- kNN(data, k = 5)
data_imputed[mv_cols, c(1,2,3,4,5)]
```

Perfecte!!

```{r}
data$CHAS <- data_imputed$CHAS
summary(data$CHAS) #   :)
```

```{r}
#convert to factor
data$CHAS <- factor(data$CHAS)
```

```{r}

for (i in 1:14){
  if (i==4) next
  hist(data[,i],main= paste(colnames(data)[i]), breaks = 15) #vaya distribucions de merda ... xd
  print(colnames(data)[i])
  print(shapiro.test(data[,i]))
}
```

```{r}
library(Hmisc)
#hist.data.frame(data)
```

## Box-Cox tranformation using optimal lambda

```{r}
library(MASS)
library(car)
library(VGAM)

data.BC <- data.frame(matrix(ncol = ncol(data), nrow = nrow(data)))
colnames(data.BC) <- colnames(data)
for (i in 1:14){
  if (i==2 || i==4) next
  #find best lambda for BoxCox transformation
  boxcox_result <- boxcox(data[,i] ~ 1, lambda = seq(-5, 5, by = 0.1))
  lambda_optimal <- boxcox_result$x[which.max(boxcox_result$y)]
  print(lambda_optimal)
  
  #apply BoxCox transformation
  name = colnames(data)[i]
  if(lambda_optimal==0){
    data.BC[,i] <- log(data[,i])
  } else {
    data.BC[,i] <- yeo.johnson(data[,i], lambda=lambda_optimal)
  }
  
  hist(data.BC[,i], main= paste(colnames(data.BC)[i]), breaks = 15)
  print(colnames(data.BC)[i])
  print(shapiro.test(data.BC[,i]))
}

data.BC$ZN <- data$ZN
data.BC$CHAS <- data$CHAS
```

Seguim tenint una normalitat bastant bastant pèssima...

```{r}
library(corrplot)
corrplot(cor(data[, colnames(data)!=c("CHAS")]))
```

## Outliers treatment

```{r}
for (i in 1:14){
  if (i==4) next
  boxplot(data[,i],main= paste(colnames(data.BC)[i])) #vaya distribucions de   merda ... xd
}
```

```{r}
library(ggplot2)
library(reshape)
meltdata <- melt(data.BC)
p <- ggplot(meltdata, aes(factor(variable), value)) 
p + geom_boxplot() + facet_wrap(~variable, scale="free")

```

B no hauríem de considerar treure els outliers...

```{r}
total = 0
indexs = c()
for (i in 1:14){
  if (i %in% c(4,12)) next

  lower_bound <- quantile(data.BC[,i], 0.025)
  upper_bound <- quantile(data.BC[,i], 0.975)
  outlier_ind <- which(data.BC[,i] < lower_bound | data.BC[,i] > upper_bound)
  print(colnames(data.BC)[i])
  print((sum(outlier_ind/outlier_ind)/506)*100)
  
  indexs = union(indexs, outlier_ind)
}

print(indexs)
print(length(indexs))
```

Només tenim 506 files en total, són massa outliers per treurel's tots!!! proseguirem a l'anàlisi tenint en compte això, i a menys que ho trobem necessari, utilitzarem el dataset sense treure els outliers.

```{r}
data.BC.outl <- data[-indexs,]
```

```{r}
corrplot(cor(data[, colnames(data)!=c("ZN", "CHAS")]))
#quasi igual que abans
```

# Principal Components Analysis

```{r}
library(HSAUR)

pca <- princomp(data.BC[, colnames(data.BC)!=c("CHAS")], cor=TRUE)
print("PCA whithout outliers treatment")
summary(pca)



print("_______________________________________________________")
print(".......................................................")
print("_______________________________________________________")
print("PCA with outliers treatment")
pca.outl <- princomp(data.BC.outl[, colnames(data)!=c("CHAS")],cor=TRUE)
summary(pca.outl)

# necessitariem més dimensions... ja ens va bé no treure els outliers.
```

```{r}
biplot(pca)
points(data.BC, pch = 16)
```

```{r}
pca$eigs > 1
```

```{r}
fviz_eig(pca, barcolor = "gray40",barfill = "gray67")
```

```{r}
library(ggfortify)
autoplot(pca, data=data.BC, colour = "CHAS", loadings=TRUE,x = 1,y=2,loadings.label = TRUE,loadings.colour = rep("gray33", times=13), 
         loadings.label.colour=rep("black", times=13))

autoplot(pca, data=data.BC, colour = "CHAS", loadings=TRUE,x = 1, y=3,loadings.label = TRUE, loadings.colour = rep("gray33", times=13), 
         loadings.label.colour=rep("black", times=13))
```

```{r}
fviz_pca_var(pca,
             col.var = "cos2", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE,     # Avoid text overlapping
             axes = c(1, 3) 
             )
```

```{r}
library(ggplot2)
library(viridis)

gradient_colors <- viridis(100)
palette <- scale_color_gradientn(colours = gradient_colors)

autoplot(pca, data=data.BC, colour = "CRIM", loadings=TRUE, loadings.label = TRUE) +
  palette
```

```{r}
gradient_colors <- viridis(100)

palette <- scale_color_gradientn(colours = gradient_colors)

autoplot(pca, data=data.BC, colour = "RM", loadings=TRUE, loadings.label = TRUE) +
  palette
```

```{r}
gradient_colors <- viridis(100)
palette <- scale_color_gradientn(colours = gradient_colors)

autoplot(pca, data=data.BC, colour = "TAX", loadings=TRUE, loadings.label = TRUE) +
  palette
```

```{r}

gradient_colors <- viridis(100)
palette <- scale_color_gradientn(colours = gradient_colors)

autoplot(pca, data=data.BC, colour = "AGE", loadings=TRUE, loadings.label = TRUE) +
  palette

```

## PCA to factor computations

```{r}
pca$eigs <- pca$sdev^2
pca$eigs
```

```{r}
# aij=sqrt(eig_j)*u_ji
# A = [aij]
A <- matrix(0, nrow = nrow(pca$loadings), ncol = 4)
for (i in 1:nrow(pca$loadings)) {
  for (j in 1:4) {
    A[i,j] <- pca$loadings[i, j] * sqrt(pca$eigs[j])
  }
}

# Convertim el resultat en un dataframe
A <- as.data.frame(A)
colnames(A) <- paste0("Dim", 1:4)
rownames(A) <- rownames(pca$loadings)

# Mostrem el dataframe
print(A)
```

## Factor correlations

```{r}
components <- matrix(NA, nrow = ncol(pca$loadings), ncol = 4)

for (i in 1:4) {
  components[, i] <- pca$loadings[, i] * sqrt(pca$sdev[i])
}

components_df <- as.data.frame(components)

rownames(components_df) <- rownames(pca$loadings)
colnames(components_df) <- paste0("Dim", 1:4)

print(components_df)
```

## Comunalities

```{r}
comunalitats <- matrix(0, nrow = nrow(pca$loadings), ncol = 4)
for (i in 1:4) {
  comunalitats[, i] <- (pca$loadings[, i] * sqrt(pca$eigs[i]))^2
}
comunalitats_totals <- rowSums(comunalitats)
comunalitats_df <- as.data.frame(comunalitats)
colnames(comunalitats_df) <- paste0("Dim", 1:4)
rownames(comunalitats_df) <- rownames(pca$loadings)
print(comunalitats_df)
comunalitats_totals_df <- as.data.frame(comunalitats_totals)
rownames(comunalitats_totals_df) <- rownames(pca$loadings)
print(comunalitats_totals_df)
```

```{r}
names(data.BC)
```

# MDS

```{r}
library(ade4)
data.scaled <- scale(data.BC[, colnames(data)!=c("CHAS")])
data.scale.dist <- dist(data.scaled) #euclidean distance

mds <- dudi.pco(data.scale.dist, scannf=FALSE, nf=10)

names(mds)
summary(mds)
```

```{r}
num_eigenvalues <- 10
eigenvalues <- mds_boston$eig[1:num_eigenvalues]
eigenvalues_df <- data.frame(Dimension = 1:num_eigenvalues, Eigenvalue = eigenvalues)

# Scree Plot amb ggplot2
ggplot(eigenvalues_df, aes(x = Dimension, y = Eigenvalue)) +
  geom_point(size = 3) +
  geom_line() +
  scale_x_continuous(breaks = 1:num_eigenvalues) +
  labs(title = "Scree Plot",
       x = "Dimensions",
       y = "Eigenvalue") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}

```

```{r}
mds_data <- data.frame(Coordinate1 = mds$li[, 1],
                       Coordinate2 = mds$li[, 2],
                       var = data.BC$MEDV)

# Fer el plot amb ggplot2
ggplot(mds_data, aes(x = Coordinate1, y = Coordinate2, color = var)) +
  geom_point() +
  scale_color_gradient(low = "blue", high = "red") +
  labs(title = "MDS on Boston Housing Data",
       x = "Coordinate 1",
       y = "Coordinate 2",
       color = "MEDV") +
  theme_minimal()
```

```{r}
mds_data <- data.frame(Coordinate1 = mds$li[, 1],
                       Coordinate2 = mds$li[, 3],
                       var = data.BC$CHAS)

# Fer el plot amb ggplot2
ggplot(mds_data, aes(x = Coordinate1, y = Coordinate2, color = var)) +
  geom_point() +
  labs(title = "MDS on Boston Housing Data",
       x = "Coordinate 1",
       y = "Coordinate 2",
       color = "CHAS") +
  theme_minimal() +
  scale_color_discrete(name = "CHAS")
```

```{r}
mds_data <- data.frame(Coordinate1 = mds$li[, 1],
                       Coordinate2 = mds$li[, 3],
                       var = data.BC$MEDV)

# Fer el plot amb ggplot2
ggplot(mds_data, aes(x = Coordinate1, y = Coordinate2, color = var)) +
  geom_point() +
  scale_color_gradient(low = "blue", high = "red") +
  labs(title = "MDS on Boston Housing Data",
       x = "Coordinate 1",
       y = "Coordinate 2",
       color = "MEDV") +
  theme_minimal()
```

# Multiple Correspondance Analysis

We chose the variables INDUS, RM, RAD, TAX, PTRATIO and LSTAT for this MCA. Thus, we have to convert them to categorical variables, we will use the quartiles as partitions for three groups: low, normal and high.
```{r}
set.seed(1)
# We calculate the quartiles for INDUS
indus_quartiles <- quantile(data.BC$INDUS, probs = c(0.25, 0.75))

# We use the cut function to categorize INDUS into low, normal, and high
data.BC$Cat_INDUS <- cut(data.BC$INDUS,
                    breaks = c(-Inf, indus_quartiles[1], indus_quartiles[2], Inf),
                    labels = c("low", "normal", "high"),
                    right = TRUE)


# We calculate the quartiles for RM
RM_quartiles <- quantile(data.BC$RM, probs = c(0.25, 0.75))

# We use the cut function to categorize RM into low, normal, and high
data.BC$Cat_RM <- cut(data.BC$RM,
                    breaks = c(-Inf, RM_quartiles[1], RM_quartiles[2], Inf),
                    labels = c("low", "normal", "high"),
                    right = TRUE)

# We calculate the quartiles for RAD
RAD_quartiles <- quantile(data.BC$RAD, probs = c(0.25, 0.6))

# We use the cut function to categorize RAD into low, normal, and high
data.BC$Cat_RAD <- cut(data.BC$RAD,
                    breaks = c(-Inf, RAD_quartiles[1], RAD_quartiles[2], Inf),
                    labels = c("low", "normal", "high"),
                    right = TRUE)


# We calculate the quartiles for TAX
TAX_quartiles <- quantile(data.BC$TAX, probs = c(0.25, 0.75))

# We use the cut function to categorize TAX into low, normal, and high
data.BC$Cat_TAX <- cut(data.BC$TAX,
                    breaks = c(-Inf, TAX_quartiles[1], TAX_quartiles[2], Inf),
                    labels = c("low", "normal", "high"),
                    right = TRUE)

# We calculate the quartiles for PTRATIO
PTRATIO_quartiles <- quantile(data.BC$PTRATIO, probs = c(0.25, 0.75))

# We use the cut function to categorize PTRATIO into low, normal, and high
data.BC$Cat_PTRATIO <- cut(data.BC$PTRATIO,
                    breaks = c(-Inf, PTRATIO_quartiles[1], PTRATIO_quartiles[2], Inf),
                    labels = c("low", "normal", "high"),
                    right = TRUE)

# We calculate the quartiles for LSTAT
LSTAT_quartiles <- quantile(data.BC$LSTAT, probs = c(0.25, 0.75))

# We use the cut function to categorize LSTAT into low, normal, and high
data.BC$Cat_LSTAT <- cut(data.BC$LSTAT,
                    breaks = c(-Inf, LSTAT_quartiles[1], LSTAT_quartiles[2], Inf),
                    labels = c("low", "normal", "high"),
                    right = TRUE)


# View the updated dataset
head(data.BC)
```
```{r}
## Indicator Matrix
library(FactoMineR)
tab.disjonctif(data.BC[1:10,15:20])
```
## MCA Application on Data using Indicator Matrix
```{r}
set.seed(2)
mca <- MCA(data.BC[,15:20])
```
```{r}
plot(mca, axes = c(1, 3), col.var = "blue", col.ind = "red", title = "Factor Map: Dimension 1 vs Dimension 3")
```



```{r}
summary(mca)
```
We choose the dimensions with eigenvalues >1/6 = 0.167, and this are the first 5 dimensions.

```{r}
summary(mca, ncp=5)
```

```{r}
## Coordinates of categories
mca$var$coord
```
## Cloud of Individuals
```{r}
set.seed(3)
plot.MCA(mca,choix="ind",label="none")
plot.MCA(mca,choix="ind",label="none",invisible="var")
plot.MCA(mca,choix="ind",label="none",invisible="var",habillage = "Cat_INDUS")
plot.MCA(mca,choix="ind",label="none",invisible="var",habillage = "Cat_TAX")
plot.MCA(mca,choix="ind",label="none",invisible="var",habillage = "Cat_PTRATIO")
plot.MCA(mca,choix="ind",label="none",invisible="var",habillage = "Cat_RM")
plot.MCA(mca,choix="ind",label="none",invisible="var",habillage = "Cat_LSTAT")
plot.MCA(mca,choix="ind",label="none",invisible="var",habillage = "Cat_RAD")
```
## Cloud of Categories

```{r}
plot(mca,invisible=c("ind"),title="Graph of the active categories")
rownames(mca$var$coord)
plot(mca,invisible=c("ind"),col.var=c(rep(c("black","red"),17),"black",rep("red",4)),title="Graph of the active categories")
```
```{r}
head(data.BC)
```

## Cloud of variables
```{r}
plot(mca,choix="var",title="Cloud of variables")
```
```{r}
#We eliminate the 6 extra categorical variables we created, 
#to be able to work with the original dataset for the next methods.
set.seed(4)
data.BC<- data.BC[, 1:14]
data.BC.CA <- subset(data.BC, select = -ZN)
data.BC.CA <- subset(data.BC.CA, select = -CHAS)
data.BC.CA <- subset(data.BC.CA, select = -B)
head(data.BC)
head(data.BC.CA)
```

## Cluster Analysis
```{r}
fc <- scale(data.BC.CA)
fc <- as.data.frame(fc)
head(fc)
d <- dist(fc, method = "euclidean") # distance matrix
d
```
##We aply hierarchical cluster analysis and plot the dendograms.

```{r}
fit <- hclust(d, method="single") 
plot(fit,main="Dendrogram of Single Linkage")
groups <- cutree(fit, k=2 )# c
rect.hclust(fit, k=2, border="green")

fit1 <- hclust(d, method="complete") 
plot(fit1,main="Dendrogram of complete Linkage") # Dendogram 
groups <- cutree(fit1, k=2 )# c
rect.hclust(fit1, k=2, border="green")

fit2 <- hclust(d, method="average") 
plot(fit2,main="Dendrogram of Average Linkage") # Dendogram 
groups <- cutree(fit2, k=2 )# c
rect.hclust(fit2, k=2, border="green")

fit3 <- hclust(d, method="ward.D2") 
plot(fit3,main="Dendrogram of Ward Method") # Dendogram 
groups <- cutree(fit3, k=2 )# c
rect.hclust(fit3, k=2, border="green")

fit4 <- hclust(d, method="centroid") 
plot(fit4,main="Dendrogram of Centroid Method") # Dendogram
groups <- cutree(fit4, k=2 )# c
rect.hclust(fit4, k=2, border="green")
```
```{r}
aux<-c()
for (i in 2:6){
  k<-kmeans(fc,centers=i,nstart=3)
  aux[i-1]<-k$tot.withinss
}
plot(aux, xlab="Number of Clusters", ylab="TWSS", type="l", main="TWSS vs. number of clusters")

aux<-c()

for (i in 2:10){
  k<-kmeans(fc,centers=i,nstart=25)
  aux[i]<-((k$betweenss)*(nrow(fc)-i))/((k$tot.withinss)*(i-1))
}

plot(aux, xlab="Number of Clusters", ylab="Pseudo-F", type="l", main="Pseudo F Index")
```

```{r}
k2 <- kmeans(fc, centers = 2, nstart = 25)
str(k2)
names(k2)
```

```{r}
aggregate(data[, !(names(data.BC) %in% c("CHAS","B","ZN"))],by=list(k2$cluster),FUN=mean)
data$cluster<-as.numeric(k2$cluster)
data$cluster
```

```{r}
cluster_counts <- table(data$cluster)

# Print the counts
print(cluster_counts)
```

# Discriminant Analysis
First we import the necesssary packages:
```{r}
library(biotools)
library(DescTools)
library(MASS)
library(klaR)
library(e1071)
library(caTools)
library(caret)
library("DescTools")
library(readxl)

```
In our dataset we find that it is not very logical to separate our houses between ('CHAS': next to river/ not next to river), so we are going to apply the barlett test to analyse the homogeneity between all the possible groups. See bibliography.
```{r}
char_vars <- sapply(data, is.factor)

# Excluir variables de tipo caracter
numeric_data <- data[!char_vars]

# Realizar la prueba de Bartlett
bartlett.test(numeric_data)
```
We see that our p-value is very low, so we cannot assume homogeneity. Nevertheless, we are going to apply the other tests. We have tried the boxM test for the categories 'CHAS' and the result was not good either.

In previous sections of the project we have seen that our data does not have normality, but our transformations to the data let us at least work with this dataset, so we are going to assume we have normality even though our results will not be 100% trustworthy.

The thing here is that we have to define the classification groups before classifying, while in clustering these classification groups were simply generated by the computations. Here, we need a logical way to separate our data, and then we will be able to see which variables give us more information in order to classify. As our target is numeric, we are going to separate it with the quartiles, but before applying LDA, we are going to apply MANOVA to discard the redundant variables.

We first check the boxM again just in case the result now was good:
```{r}
quartiles <- quantile(data$MEDV, probs = c(0, 0.25, 0.5, 0.75, 1))

data$Quartile <- cut(data$MEDV, breaks = quartiles, labels = FALSE)
numeric_data$Quartile <- cut(data$MEDV, breaks = quartiles, labels = FALSE)

na_count <- sum(is.na(numeric_data$Quartile))

print(na_count)

data_sin_na <- data[complete.cases(data),]

print(colnames(data_sin_na))

num_data_na <- numeric_data[complete.cases(numeric_data),]

boxM(num_data_na[,1:13],num_data_na$Quartile)
```
Is not good either, but we will work nevertheless.

Let's apply MANOVA

```{r}
data_manova <- manova(cbind(CRIM,ZN,INDUS,NOX,RM,AGE,DIS,RAD,TAX,PTRATIO,B,LSTAT) ~ MEDV, data = data_sin_na)
summary.aov(data_manova)
```

In this case we find that all the variables explain our target variable. The most likely interpretation we think would be that all the factors are quite different by context, so everything gives us information. In other words, there is no redundance in getting the quantity of pollution in the air (NOX) and if the house is next to a river(CHAS).


```{r}
data_lda <- lda(Quartile~CRIM+ZN+INDUS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT+MEDV+CHAS, data=data_sin_na)
data_lda
```

```{r}
datapred <- predict(data_lda)
names(data_lda)
names(datapred)
datapred$class
datapred$posterior
```

contingency table of observed and predicted:

```{r}
cont_tab <- table(data_sin_na$Quartile,datapred$class)
cont_tab
```
```{r}
classrate <- sum(diag(cont_tab)/sum(cont_tab))
classrate
total_ccr <- sum(diag(prop.table((cont_tab))))
total_ccr
data_lda$prior
comparison <- cbind(data_sin_na$Quartile,datapred$class)
comparison
```

```{r}
plot(data_lda)
```
There is actually no point in doing multigroup analysis, because we are only predicting the price, and we have already seen what we need. In other cases we would have wanted to get the quantity of information that the different variables would have given us to classify between price and happiness of the people (for example), but this is not the case. Apart from that, the variables are independent with each other, as MANOVA showed us, so it is probable that the information that they give us between price and price + any other variable, is the same one.

Let's apply Naive Bayes

```{r}
nb.data <- naiveBayes(Quartile ~ CRIM,ZN,INDUS,NOX,RM,AGE,DIS,RAD,TAX,PTRATIO,B,LSTAT,CHAS, data = data_sin_na )
nb.data
```
```{r}
nb.class <- predict (nb.data, data_sin_na)
nb.data
tabnb <- table(nb.class, data_sin_na$Quartile)
tabnb

mean(nb.class == data_sin_na$Quartile)
nb.preds <- predict(nb.data, data_sin_na,type="raw")
nb.preds[1:10,]
```
#validation
```{r}
split <- sample.split(data_sin_na, SplitRatio = 0.67)
train_data<- subset(data_sin_na, split == "TRUE")
test_data <- subset(data_sin_na, split == "FALSE")

set.seed(120)  # Setting Seed
naive_data <- naiveBayes(Quartile ~ ., data = train_data)
naive_data
```
```{r}
ypred <- predict(naive_data, newdata = test_data)
conf_m <- table(test_data$Quartile, ypred)
conf_m
Conf(conf_m)
```
### Predictive Accuracy ####
```{r}
ny<-sum(diag(cont_tab))
n<-nrow(data_sin_na)
k<-2
Qlda<-((n-ny*k)^2)/(n*(k-1))
Qlda

nyb<-sum(diag(tabnb))
Qb<-((n-nyb*k)^2)/(n*(k-1))
Qb  
